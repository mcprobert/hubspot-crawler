# HubSpot Crawler v1.7.0 - Safe Restart Guide

**Date:** 2025-10-08
**Version:** 1.7.0 (All deadlock fixes applied)
**Status:** âœ… Data verified - Safe to restart

---

## ðŸ“Š Data Integrity Verification

### âœ… No Data Loss Confirmed

**Original Input:**
- File: `uk-companies-0.txt`
- Total lines: 999,999 URLs
- Unique URLs: 999,538 URLs
- Duplicates: 461 URLs (auto-deduplicated by CLI)

**Progress Before Deadlock:**
- Completed: 78,863 URLs (7.9%)
- Failed: 28,386 URLs (2.8%)
- Total processed: 107,249 URLs (10.7%)

**Checkpoint Integrity:**
- âœ… No duplicates in checkpoint.txt
- âœ… No overlap with remaining URLs
- âœ… All 78,863 completed URLs verified

**Remaining Work:**
- New file: `uk-companies-remaining-v1.7.0.txt`
- URLs: 920,674 (92.1%)
- âœ… Verified no overlap with checkpoint
- âœ… No data loss

**Verification Commands:**
```bash
# Check file counts
wc -l uk-companies-0.txt                    # 999,999 (original)
wc -l checkpoint.txt                         # 78,863 (completed)
wc -l failures.jsonl                         # 28,386 (failed)
wc -l uk-companies-remaining-v1.7.0.txt     # 920,674 (remaining)

# Verify no duplicates in checkpoint
sort checkpoint.txt | uniq -d | wc -l       # Should be 0

# Verify no overlap
comm -12 <(sort checkpoint.txt) <(sort uk-companies-remaining-v1.7.0.txt) | wc -l  # Should be 0
```

---

## ðŸš€ Quick Start - Restart Crawler

### Option 1: Interactive Script (Recommended)
```bash
./RESTART_v1.7.0.sh
```

**Features:**
- Interactive mode selection (ultra-conservative to aggressive)
- Block detection configuration
- Output format selection (JSONL/CSV/Excel)
- Progress tracking
- Automatic backup of existing results
- Estimated completion time

### Option 2: Manual Command

**Ultra-Conservative (Recommended for 900k URLs):**
```bash
python3 -m hubspot_crawler.cli \
  --input uk-companies-remaining-v1.7.0.txt \
  --out results.jsonl \
  --output-format jsonl \
  --mode ultra-conservative \
  --checkpoint checkpoint.txt \
  --failures failures.jsonl \
  --max-retries 3 \
  --try-variations \
  --max-variations 4 \
  --progress-style detailed \
  --progress-interval 10 \
  --block-detection \
  --block-action pause \
  --block-threshold 5 \
  --block-window 20 \
  --block-auto-resume 300
```

**Conservative (Faster, still safe):**
```bash
python3 -m hubspot_crawler.cli \
  --input uk-companies-remaining-v1.7.0.txt \
  --out results.jsonl \
  --mode conservative \
  --checkpoint checkpoint.txt \
  --failures failures.jsonl \
  --max-retries 3 \
  --try-variations \
  --progress-style detailed \
  --block-detection \
  --block-action warn
```

---

## ðŸ“ˆ Real-Time Monitoring

### Monitor Script (Auto-detects stalls)
```bash
# In separate terminal
./monitor_v1.7.0.py

# With custom settings
./monitor_v1.7.0.py --stall-threshold 15 --interval 30 --total 920674
```

**Features:**
- Real-time progress tracking
- Stall detection (alerts if no progress for 10+ minutes)
- ETA calculation
- URLs/sec rate monitoring
- Last update timestamp
- Auto-refresh every 30 seconds

### Manual Monitoring
```bash
# Watch checkpoint grow
watch -n 10 'wc -l checkpoint.txt'

# Monitor with stats
watch -n 30 'echo "Checkpoint: $(wc -l < checkpoint.txt) | Failures: $(wc -l < failures.jsonl) | Total: $(($(wc -l < checkpoint.txt) + $(wc -l < failures.jsonl)))"'

# Check process status
ps aux | grep hubspot_crawler

# Check for stalls (no file updates in 10+ min)
ls -lh checkpoint.txt failures.jsonl
```

---

## ðŸ›¡ï¸ v1.7.0 Improvements

### Deadlock Fixes Applied
1. âœ… Unbounded detector_queue (no pause deadlock)
2. âœ… Variation loop timeout (300s auto-resume)
3. âœ… Writer health monitoring (fail-fast on disk errors)
4. âœ… Shutdown during pause (graceful cleanup)
5. âœ… Coordinator cleanup (try/finally ensures resume)
6. âœ… TLS verification enabled (--insecure to disable)
7. âœ… Removed broken retry_urls_queue
8. âœ… Pause in retry loop (block detection works)
9. âœ… Blocking I/O to threads (Excel/CSV/checkpoint)
10. âœ… Worker auto-resume race removed
11. âœ… Quiet mode output fixed
12. âœ… Numeric validation added

### What Changed
- **No more deadlocks** - 9 vectors eliminated
- **Fail-fast errors** - disk/permission issues surface immediately
- **Security hardened** - TLS verification on by default
- **Clean codebase** - removed broken features
- **All 239 tests passing**

---

## ðŸ” Troubleshooting

### If Crawler Stalls Again

**1. Check Process Status:**
```bash
ps aux | grep hubspot_crawler
# Look for 0% CPU - indicates potential issue
```

**2. Check File Updates:**
```bash
ls -lh checkpoint.txt failures.jsonl
# If modified time is >10 minutes old, potential stall
```

**3. Check Disk Space:**
```bash
df -h .
# Ensure sufficient space for output files
```

**4. Kill if Necessary:**
```bash
# Find process
ps aux | grep hubspot_crawler

# Kill gracefully
kill <PID>

# Force kill if needed (last resort)
kill -9 <PID>
```

**5. Restart from Checkpoint:**
```bash
# Checkpoint will resume from where it left off
./RESTART_v1.7.0.sh
```

### Common Issues

**Stall During Pause:**
- v1.7.0 has 300s timeout - auto-resumes
- Check block detection settings
- Use `--block-action warn` for headless

**Disk Full:**
- Check `df -h`
- Move/compress old results
- Checkpoint allows restart

**Permission Errors:**
- v1.7.0 fails fast with clear error
- Check file permissions
- Ensure write access to checkpoint/output

**TLS Errors:**
- New in v1.7.0: TLS enabled by default
- Use `--insecure` if needed (not recommended)
- Check network/proxy settings

---

## ðŸ“Š Expected Performance

### Completion Times (920k URLs)

**Ultra-Conservative Mode (Default):**
- Concurrency: 2
- Delay: 3s + 1s jitter
- Rate: ~0.5 URLs/sec (1,800/hour)
- Time: ~460 hours (19 days)
- Risk: Virtually zero block risk

**Conservative Mode:**
- Concurrency: 5
- Delay: 1s + 0.3s jitter
- Rate: ~4 URLs/sec (14,400/hour)
- Time: ~54 hours (2.3 days)
- Risk: Minimal

**Balanced Mode:**
- Concurrency: 10
- Delay: 0.5s + 0.2s jitter
- Rate: ~12 URLs/sec (43,200/hour)
- Time: ~18 hours
- Risk: Low-medium

**Aggressive Mode:**
- Concurrency: 20
- Delay: 0s + 0s jitter
- Rate: ~20 URLs/sec (72,000/hour)
- Time: ~12 hours
- Risk: HIGH (block detection strongly recommended)

---

## ðŸŽ¯ Recommended Strategy

### For 920k Remaining URLs

**Phase 1: Conservative Start (First 50k URLs)**
```bash
# Test v1.7.0 fixes with conservative mode
./RESTART_v1.7.0.sh
# Select: Mode 2 (Conservative), Block detection enabled
```

**Monitor for 2-3 hours:**
- Watch for stalls (should not occur in v1.7.0)
- Check block detection alerts
- Verify checkpoint updates regularly

**Phase 2: Scale Up (If stable)**
```bash
# After verifying stability, increase to balanced mode
# Kill current crawl (Ctrl+C)
# Restart with mode 3 (Balanced)
./RESTART_v1.7.0.sh
```

**Phase 3: Final Push (If needed)**
```bash
# For last 100k URLs, can use aggressive mode if time-sensitive
# Ensure block detection is enabled
./RESTART_v1.7.0.sh
# Select: Mode 4 (Aggressive), Block detection REQUIRED
```

---

## âœ… Success Criteria

**Crawl Completed Successfully When:**
1. `wc -l checkpoint.txt` â‰ˆ 999,538 (all unique URLs)
2. No stalls detected by monitor
3. Results file created successfully
4. No unhandled errors in output

**Verification:**
```bash
# Total processed should equal unique URLs
echo "Completed: $(wc -l < checkpoint.txt)"
echo "Failed: $(wc -l < failures.jsonl)"
echo "Total processed: $(($(wc -l < checkpoint.txt) + $(wc -l < failures.jsonl)))"
echo "Expected: 999,538"

# Check results file
wc -l results.jsonl  # Should match checkpoint
head -1 results.jsonl | python3 -m json.tool  # Verify format
```

---

## ðŸ†˜ Emergency Procedures

### If Major Issue Occurs

**1. Immediate Actions:**
```bash
# Kill crawler
pkill -f hubspot_crawler

# Backup current state
cp checkpoint.txt checkpoint.backup-$(date +%Y%m%d-%H%M%S).txt
cp failures.jsonl failures.backup-$(date +%Y%m%d-%H%M%S).jsonl
```

**2. Verify Data:**
```bash
# Check checkpoint integrity
wc -l checkpoint.txt
sort checkpoint.txt | uniq -d | wc -l  # Should be 0

# Recreate remaining URLs if needed
python3 -c "
completed = set(open('checkpoint.txt').read().splitlines())
original = open('uk-companies-0.txt').read().splitlines()
remaining = [u for u in original if u not in completed]
open('uk-companies-remaining-recovery.txt', 'w').write('\\n'.join(remaining))
print(f'Created recovery file with {len(remaining)} URLs')
"
```

**3. Contact/Report:**
- Check GitHub issues: https://github.com/mcprobert/hubspot-crawler/issues
- Include: checkpoint size, error messages, monitor output
- v1.7.0 should not deadlock - report if it does!

---

## ðŸ“ Monitoring Checklist

**Every Hour:**
- [ ] Check monitor output
- [ ] Verify checkpoint growing
- [ ] Check disk space
- [ ] Review any block detection alerts

**Every 6 Hours:**
- [ ] Compare progress to ETA
- [ ] Verify no stalls detected
- [ ] Check failure rate (<5% is normal)
- [ ] Review block detection stats

**Daily:**
- [ ] Backup checkpoint file
- [ ] Review total progress
- [ ] Adjust mode if needed
- [ ] Verify v1.7.0 stability

---

## ðŸŽ‰ Expected Outcome

**With v1.7.0 Fixes:**
- âœ… No deadlocks (9 vectors eliminated)
- âœ… Stable multi-day crawls
- âœ… Automatic stall detection
- âœ… Graceful error handling
- âœ… Complete data capture
- âœ… Safe resume from any point

**Final Results:**
- 999,538 URLs processed (100%)
- Comprehensive HubSpot detection data
- Failed URLs tracked for investigation
- Ready for analysis/import

---

**Good luck with the crawl! The v1.7.0 fixes ensure you won't lose any more data.**
